{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f93475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Compose, Normalize, RandomHorizontalFlip, Resize, ToTensor\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b071d44",
   "metadata": {},
   "source": [
    "### Patch Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0ae40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=786):\n",
    "        super(PatchEmbed, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        self.proj = nn.Conv2d(in_channels=in_chans,\n",
    "                              out_channels=embed_dim,\n",
    "                              kernel_size=patch_size,\n",
    "                              stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        x = x.flatten(2) \n",
    "        x = x.transpose(1,2) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b773a63",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31de66cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, embed_dim=768, head_dim=768, attn_p=0):\n",
    "        super(Head, self).__init__()\n",
    "        self.query = nn.Linear(embed_dim, head_dim)\n",
    "        self.key = nn.Linear(embed_dim, head_dim)\n",
    "        self.value = nn.Linear(embed_dim, head_dim)\n",
    "        self.attn_dropout = nn.Dropout(attn_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, n_patch, embed_dim = x.shape\n",
    "        q = self.query(x) \n",
    "        k = self.key(x)\n",
    "        v = self.value(x) \n",
    "\n",
    "        sam = (q @ k.transpose(-2,-1)) * embed_dim**-0.5 \n",
    "        attn = sam.softmax(dim=-1)\n",
    "        attn = self.attn_dropout(attn)\n",
    "        weighted_average = attn @ v \n",
    "        return weighted_average\n",
    "    \n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, embed_dim=768, num_heads=12, attn_p=0, proj_p=0):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        self.head_size = embed_dim // num_heads\n",
    "        self.heads = nn.ModuleList([Head(embed_dim=embed_dim, head_dim=self.head_size, attn_p=attn_p) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.proj_drop = nn.Dropout(proj_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) \n",
    "        out = self.proj_drop(self.proj(out)) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e87e5a",
   "metadata": {},
   "source": [
    "### Efficient Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b615b79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, attn_p, proj_p):\n",
    "        super(EfficientAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = int(self.embed_dim / num_heads)\n",
    "\n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim*3)\n",
    "        self.attn_dropout = nn.Dropout(attn_p)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.proj_drop = nn.Dropout(proj_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, patches, embed_dim = x.shape \n",
    "        qkv = self.qkv(x)\n",
    "        qkv = qkv.reshape(batch, patches, 3, self.num_heads, self.head_size) \n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)  \n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        sam = (q @ k.transpose(-2,-1)) * self.head_size**-0.5 \n",
    "        attn = sam.softmax(dim=-1)\n",
    "        attn = self.attn_dropout(attn)\n",
    "        weighted_average = attn @ v \n",
    "        weighted_average = weighted_average.transpose(1,2) \n",
    "        weighted_average = weighted_average.flatten(2) \n",
    "        out = self.proj_drop(self.proj(weighted_average))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9f7bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features, mlp_p=0):\n",
    "        \n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(mlp_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.fc1(x)) \n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x) \n",
    "        x = self.drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0bdc22",
   "metadata": {},
   "source": [
    "### Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11258d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,embed_dim= 768, num_heads= 12, mlp_ratio = 4.0, mlp_p= 0, attn_p = 0,proj_p = 0,efficient = True):\n",
    "        super(TransformerBlock,self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim,eps = 1e-6)\n",
    "\n",
    "        if efficient:\n",
    "            self.attn = EfficientAttention(\n",
    "                embed_dim= embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                attn_p=attn_p,\n",
    "                proj_p=proj_p,\n",
    "            )\n",
    "        else:\n",
    "            self.attn = MultiHeadedAttention(\n",
    "                embed_dim= embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                attn_p=attn_p,\n",
    "                proj_p=proj_p\n",
    "            )\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(embed_dim,eps = 1e-6)\n",
    "        hidden_features = int(embed_dim*mlp_ratio)\n",
    "        self.mlp = MLP(\n",
    "            in_features= embed_dim,\n",
    "            hidden_features= hidden_features,\n",
    "            out_features=embed_dim,\n",
    "            mlp_p=mlp_p\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dd9491",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self,img_size = 224 , patch_size =16,in_chans = 3, n_classes = 2,\n",
    "                 embed_dim = 768, depth = 12, num_heads = 12, mlp_ratio = 4, attn_p = 0.2,\n",
    "                 mlp_p = 0.2, proj_p = 0.2, pos_drop = 0.2, efficient = True):\n",
    "        super(VisionTransformer,self).__init__()\n",
    "        \n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            in_chans=in_chans,\n",
    "            embed_dim=embed_dim\n",
    "        )\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1,1,embed_dim))\n",
    "        self.pos_embed = nn.Parameter((torch.zeros(1,1+self.patch_embed.n_patches,embed_dim)))\n",
    "        self.pos_drop = nn.Dropout(pos_drop)\n",
    "        \n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(\n",
    "                    embed_dim=embed_dim,\n",
    "                    num_heads=num_heads,\n",
    "                    mlp_ratio=mlp_ratio,\n",
    "                    mlp_p=mlp_p,\n",
    "                    attn_p=attn_p,\n",
    "                    proj_p=proj_p,\n",
    "                    efficient= efficient\n",
    "                )\n",
    "                for _ in range(depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.norm = nn.LayerNorm(embed_dim,eps = 1e-6)\n",
    "        self.head = nn.Linear(embed_dim,n_classes)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "        cls_token = self.cls_token.expand(batch_size,-1,-1)\n",
    "        x = torch.cat((cls_token,x),dim = 1)\n",
    "        x = x+ self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        x = self.norm(x)\n",
    "        cls_token_final = x[:,0]\n",
    "        x = self.head(cls_token_final)\n",
    "        return x,cls_token_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b83a25a",
   "metadata": {},
   "source": [
    "###  Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9086c180",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,device,epochs,optimizer,scheduler,loss_fn,train_loader,val_loader,savepath = \"ViT.pt\"):\n",
    "    log_training = {\n",
    "        \"epoch\": [],\n",
    "        \"training_loss\": [],\n",
    "        \"training_acc\": [],\n",
    "        \"validation_loss\": [],\n",
    "        \"validation_acc\": []\n",
    "    }\n",
    "\n",
    "    best_val_loss = np.inf\n",
    "    for epoch in range(1,epochs+1):\n",
    "        print(f\"Epoch: {epoch}\")\n",
    "        training_losses , training_accuracies = [],[]\n",
    "        validation_losses , validation_accuracies = [], []\n",
    "\n",
    "        for image,label in tqdm(train_loader):\n",
    "            image, label = image.to(device) , label.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out, _ = model.forward(image)\n",
    "\n",
    "            loss = loss_fn(out,label)\n",
    "            training_losses.append(loss.item())\n",
    "\n",
    "            predictions = torch.argmax(out,axis=1)\n",
    "            accuracy = (predictions==label).sum()/len(predictions)\n",
    "            training_accuracies.append(accuracy.item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        for image,label in tqdm(val_loader):\n",
    "            image,label = image.to(device), label.to(device)\n",
    "            with torch.no_grad():\n",
    "                out, _ = model.forward(image)\n",
    "\n",
    "                loss = loss_fn(out,label)\n",
    "                validation_losses.append(loss.item())\n",
    "\n",
    "                predictions = torch.argmax(out,axis = 1)\n",
    "                accuracy = (predictions==label).sum() / len(predictions)\n",
    "                validation_accuracies.append(accuracy.item())\n",
    "\n",
    "        training_loss_mean , training_acc_mean = np.mean(training_losses) , np.mean(training_accuracies)\n",
    "        validation_loss_mean,  validation_acc_mean = np.mean(validation_losses), np.mean(validation_accuracies)\n",
    "\n",
    "        if validation_loss_mean<best_val_loss:\n",
    "            print(\"----Saving Model-------\")\n",
    "            torch.save(model.state_dict(),savepath)\n",
    "            best_val_loss = validation_loss_mean\n",
    "        \n",
    "        log_training[\"epoch\"].append(epoch)\n",
    "        log_training[\"training_loss\"].append(training_loss_mean)\n",
    "        log_training[\"training_acc\"].append(training_acc_mean)\n",
    "        log_training[\"validation_loss\"].append(validation_loss_mean)\n",
    "        log_training[\"validation_acc\"].appenf(validation_acc_mean)\n",
    "\n",
    "        print(f\"Training Loss: {training_loss_mean}\")\n",
    "        print(f\"Training Acc: {training_acc_mean}\")\n",
    "        print(f\"Validation Loss: {validation_loss_mean}\")\n",
    "        print(f\"Validation Acc: {validation_acc_mean}\")\n",
    "    return log_training, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6305d9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ViT = VisionTransformer(\n",
    "    embed_dim=384,\n",
    "    depth = 6,\n",
    "    num_heads= 6,\n",
    "    efficient= True\n",
    ")\n",
    "\n",
    "params = sum([np.prod(p.size()) for p in ViT.parameters()])\n",
    "print(f\"Total Number of parameters: {params}\")\n",
    "\n",
    "path_to_data = \"./dogsvscats\"\n",
    "dataset = ImageFolder(path_to_data)\n",
    "\n",
    "normalizer = Normalize(mean = [0.485, 0.456,0.406],std = [0.229,0.224,0.225])\n",
    "train_transforms = Compose([\n",
    "    Resize((224,224)),\n",
    "    RandomHorizontalFlip(),\n",
    "    ToTensor(),\n",
    "    normalizer\n",
    "])\n",
    "\n",
    "val_transforms = Compose([\n",
    "    Resize((224,224)),\n",
    "    ToTensor(),\n",
    "    normalizer\n",
    "])\n",
    "\n",
    "train_samples , test_samples = int(0.9 * len(dataset)) , len(dataset) - int(0.9 * len(dataset))\n",
    "train_dataset , val_dataset = torch.utils.data.random_split(dataset, lengths = [train_samples,test_samples])\n",
    "\n",
    "train_dataset.dataset.transforms = train_transforms\n",
    "val_dataset.dataset.transforms = val_transforms\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "optimizer = optim.AdamW(params = ViT.parameters(),lr = 1e-3)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "batch_size = 128\n",
    "train_loader  = DataLoader(train_dataset,batch_size = batch_size, shuffle = True)\n",
    "val_loader = DataLoader(val_dataset, batch_size = batch_size, shuffle = False)\n",
    "\n",
    "epochs = 10\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps = 250,\n",
    "                                            num_training_steps = epochs * (len(train_loader))\n",
    "                                            )\n",
    "\n",
    "loss , model = train(model= ViT.to(device),\n",
    "                     device = device,\n",
    "                     epochs=epochs,\n",
    "                     optimizer=optimizer,\n",
    "                     scheduler=scheduler,\n",
    "                     loss_fn=loss_fn,\n",
    "                     train_loader=train_loader,\n",
    "                     val_loader=val_loader\n",
    "                     )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
