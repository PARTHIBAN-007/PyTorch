{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f93475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Compose, Normalize, RandomHorizontalFlip, Resize, ToTensor\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b071d44",
   "metadata": {},
   "source": [
    "### Patch Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0ae40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=786):\n",
    "        super(PatchEmbed, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        self.proj = nn.Conv2d(in_channels=in_chans,\n",
    "                              out_channels=embed_dim,\n",
    "                              kernel_size=patch_size,\n",
    "                              stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        x = x.flatten(2) \n",
    "        x = x.transpose(1,2) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b773a63",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31de66cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, embed_dim=768, head_dim=768, attn_p=0):\n",
    "        super(Head, self).__init__()\n",
    "        self.query = nn.Linear(embed_dim, head_dim)\n",
    "        self.key = nn.Linear(embed_dim, head_dim)\n",
    "        self.value = nn.Linear(embed_dim, head_dim)\n",
    "        self.attn_dropout = nn.Dropout(attn_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, n_patch, embed_dim = x.shape\n",
    "        q = self.query(x) \n",
    "        k = self.key(x)\n",
    "        v = self.value(x) \n",
    "\n",
    "        sam = (q @ k.transpose(-2,-1)) * embed_dim**-0.5 \n",
    "        attn = sam.softmax(dim=-1)\n",
    "        attn = self.attn_dropout(attn)\n",
    "        weighted_average = attn @ v \n",
    "        return weighted_average\n",
    "    \n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, embed_dim=768, num_heads=12, attn_p=0, proj_p=0):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        self.head_size = embed_dim // num_heads\n",
    "        self.heads = nn.ModuleList([Head(embed_dim=embed_dim, head_dim=self.head_size, attn_p=attn_p) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.proj_drop = nn.Dropout(proj_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) \n",
    "        out = self.proj_drop(self.proj(out)) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e87e5a",
   "metadata": {},
   "source": [
    "### Efficient Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b615b79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, attn_p, proj_p):\n",
    "        super(EfficientAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = int(self.embed_dim / num_heads)\n",
    "\n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim*3)\n",
    "        self.attn_dropout = nn.Dropout(attn_p)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.proj_drop = nn.Dropout(proj_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, patches, embed_dim = x.shape \n",
    "        qkv = self.qkv(x)\n",
    "        qkv = qkv.reshape(batch, patches, 3, self.num_heads, self.head_size) \n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)  \n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        sam = (q @ k.transpose(-2,-1)) * self.head_size**-0.5 \n",
    "        attn = sam.softmax(dim=-1)\n",
    "        attn = self.attn_dropout(attn)\n",
    "        weighted_average = attn @ v \n",
    "        weighted_average = weighted_average.transpose(1,2) \n",
    "        weighted_average = weighted_average.flatten(2) \n",
    "        out = self.proj_drop(self.proj(weighted_average))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9f7bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features, mlp_p=0):\n",
    "        \n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(mlp_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.fc1(x)) \n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x) \n",
    "        x = self.drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0bdc22",
   "metadata": {},
   "source": [
    "### Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11258d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,embed_dim= 768, num_heads= 12, mlp_ratio = 4.0, mlp_p= 0, attn_p = 0,proj_p = 0,efficient = True):\n",
    "        super(TransformerBlock,self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim,eps = 1e-6)\n",
    "\n",
    "        if efficient:\n",
    "            self.attn = EfficientAttention(\n",
    "                embed_dim= embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                attn_p=attn_p,\n",
    "                proj_p=proj_p,\n",
    "            )\n",
    "        else:\n",
    "            self.attn = MultiHeadedAttention(\n",
    "                embed_dim= embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                attn_p=attn_p,\n",
    "                proj_p=proj_p\n",
    "            )\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(embed_dim,eps = 1e-6)\n",
    "        hidden_features = int(embed_dim*mlp_ratio)\n",
    "        self.mlp = MLP(\n",
    "            in_features= embed_dim,\n",
    "            hidden_features= hidden_features,\n",
    "            out_features=embed_dim,\n",
    "            mlp_p=mlp_p\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dd9491",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self,img_size = 224 , patch_size =16,in_chans = 3, n_classes = 2,\n",
    "                 embed_dim = 768, depth = 12, num_heads = 12, mlp_ratio = 4, attn_p = 0.2,\n",
    "                 mlp_p = 0.2, proj_p = 0.2, pos_drop = 0.2, efficient = True):\n",
    "        super(VisionTransformer,self).__init__()\n",
    "        \n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            in_chans=in_chans,\n",
    "            embed_dim=embed_dim\n",
    "        )\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1,1,embed_dim))\n",
    "        self.pos_embed = nn.Parameter((torch.zeros(1,1+self.patch_embed.n_patches,embed_dim)))\n",
    "        self.pos_drop = nn.Dropout(pos_drop)\n",
    "        \n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(\n",
    "                    embed_dim=embed_dim,\n",
    "                    num_heads=num_heads,\n",
    "                    mlp_ratio=mlp_ratio,\n",
    "                    mlp_p=mlp_p,\n",
    "                    attn_p=attn_p,\n",
    "                    proj_p=proj_p,\n",
    "                    efficient= efficient\n",
    "                )\n",
    "                for _ in range(depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.norm = nn.LayerNorm(embed_dim,eps = 1e-6)\n",
    "        self.head = nn.Linear(embed_dim,n_classes)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "        cls_token = self.cls_token.expand(batch_size,-1,-1)\n",
    "        x = torch.cat((cls_token,x),dim = 1)\n",
    "        x = x+ self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        x = self.norm(x)\n",
    "        cls_token_final = x[:,0]\n",
    "        x = self.head(cls_token_final)\n",
    "        return x,cls_token_final"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
