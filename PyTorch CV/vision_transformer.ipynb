{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f93475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Compose, Normalize, RandomHorizontalFlip, Resize, ToTensor\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b071d44",
   "metadata": {},
   "source": [
    "### Patch Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0ae40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=786):\n",
    "        super(PatchEmbed, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        self.proj = nn.Conv2d(in_channels=in_chans,\n",
    "                              out_channels=embed_dim,\n",
    "                              kernel_size=patch_size,\n",
    "                              stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        x = x.flatten(2) \n",
    "        x = x.transpose(1,2) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b773a63",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31de66cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, embed_dim=768, head_dim=768, attn_p=0):\n",
    "        super(Head, self).__init__()\n",
    "        self.query = nn.Linear(embed_dim, head_dim)\n",
    "        self.key = nn.Linear(embed_dim, head_dim)\n",
    "        self.value = nn.Linear(embed_dim, head_dim)\n",
    "        self.attn_dropout = nn.Dropout(attn_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, n_patch, embed_dim = x.shape\n",
    "        q = self.query(x) \n",
    "        k = self.key(x)\n",
    "        v = self.value(x) \n",
    "\n",
    "        sam = (q @ k.transpose(-2,-1)) * embed_dim**-0.5 \n",
    "        attn = sam.softmax(dim=-1)\n",
    "        attn = self.attn_dropout(attn)\n",
    "        weighted_average = attn @ v \n",
    "        return weighted_average\n",
    "    \n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, embed_dim=768, num_heads=12, attn_p=0, proj_p=0):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        self.head_size = embed_dim // num_heads\n",
    "        self.heads = nn.ModuleList([Head(embed_dim=embed_dim, head_dim=self.head_size, attn_p=attn_p) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.proj_drop = nn.Dropout(proj_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) \n",
    "        out = self.proj_drop(self.proj(out)) \n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
