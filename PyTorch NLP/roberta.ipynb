{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c53a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaTokenizer, get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ea108b",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e732c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "\n",
    "print(\"Special Tokens in Roberta Tokenizer\")\n",
    "special_tokens = tokenizer.special_tokens_map\n",
    "print(special_tokens)\n",
    "\n",
    "\n",
    "special_token_idx = {}\n",
    "for token in special_tokens.values():\n",
    "    special_token_idx[token] = tokenizer.encode(token,add_special_tokens = False)[0]\n",
    "\n",
    "print(f\"Special Token Index: {special_token_idx}\")\n",
    "\n",
    "all_tokens_idx = list(range(tokenizer.vocab_size))\n",
    "all_special_tokens_idx = sorted(list(special_token_idx.values()))\n",
    "all_non_special_tokens_idx = [token for token in all_tokens_idx if token not in all_special_tokens_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183de5a2",
   "metadata": {},
   "source": [
    "### Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec520983",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = \"./data/harry_potter_txt/\"\n",
    "\n",
    "text_files = os.listdit(path_to_data)\n",
    "\n",
    "all_text = \"\"\n",
    "for book in text_files:\n",
    "    with open(os.path.join(path_to_data,book),\"r\") as f:\n",
    "        text = f.readlines()\n",
    "        text = [line for line in text if \"Page\" not in line]\n",
    "        text = \" \".join(text).replace(\"\\n\", \"\")\n",
    "        text = [word for word in text.split(\" \") if len(word)>0]\n",
    "        text = \" \".join(text)\n",
    "        all_text += text\n",
    "all_text = all_text.split(\".\") \n",
    "\n",
    "all_text_chunked = [\".\".join(all_text[i:i+5]) for i in range(0,len(all_text),5)]\n",
    "\n",
    "tokenized_text = [tokenizer.encode(text) for text in all_text_chunked]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06984a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLMLoader(Dataset):\n",
    "    def __init__(self, tokenized_data, max_seq_len=100, masking_ratio=0.15):\n",
    "        self.data = tokenized_data\n",
    "        self.mask_ratio = masking_ratio\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _random_mask_text(self, tokens):\n",
    "        random_masking = torch.rand(*tokens.shape)\n",
    "\n",
    "        special_tokens = torch.tensor(tokenizer.get_special_tokens_mask(tokens, already_has_special_tokens=True))\n",
    "        random_masking[special_tokens==1] = 1\n",
    "\n",
    "        random_masking = (random_masking < self.mask_ratio)\n",
    "\n",
    "        labels = torch.full((tokens.shape), -100)\n",
    "        labels[random_masking] = tokens[random_masking]\n",
    "\n",
    "        random_selected_idx = random_masking.nonzero()\n",
    "\n",
    "        masking_flag = torch.rand(*random_selected_idx.shape)\n",
    "        masking_flag = (masking_flag<0.8)\n",
    "        selected_idx_for_masking = random_selected_idx[masking_flag]\n",
    "\n",
    "        unselected_idx_for_masking = random_selected_idx[~masking_flag]\n",
    "\n",
    "        masking_flag = torch.rand(*unselected_idx_for_masking.shape)\n",
    "        masking_flag = (masking_flag<0.5)\n",
    "        selected_idx_for_random_filling = unselected_idx_for_masking[masking_flag]\n",
    "        selected_idx_to_be_left_alone = unselected_idx_for_masking[~masking_flag]\n",
    "        \n",
    "        if len(selected_idx_for_masking) > 0:\n",
    "            tokens[selected_idx_for_masking] = special_token_idx[\"<mask>\"]\n",
    "        \n",
    "        if len(selected_idx_for_random_filling) > 0:\n",
    "            randomly_selected_tokens = torch.tensor(random.sample(all_non_special_tokens_idx, len(selected_idx_for_random_filling)))\n",
    "            tokens[selected_idx_for_random_filling] = randomly_selected_tokens\n",
    "        \n",
    "        \n",
    "        return tokens, labels\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        data = torch.tensor(self.data[idx])\n",
    "\n",
    "        if len(data) > self.max_seq_len:\n",
    "            rand_start_idx = random.choice(list(range(len(data) - self.max_seq_len)))\n",
    "            end_idx = rand_start_idx + self.max_seq_len\n",
    "            data = data[rand_start_idx:end_idx]\n",
    "  \n",
    "        masked_tokens, label = self._random_mask_text(data)\n",
    "\n",
    "        return masked_tokens, label\n",
    "\n",
    "mlm = MaskedLMLoader(tokenized_text)\n",
    "\n",
    "for masked_tokens, labels in mlm:\n",
    "    print(masked_tokens)\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831250e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    token_samples = []\n",
    "    label_samples =[]\n",
    "\n",
    "    for token, label in batch:\n",
    "        token_samples.append(token)\n",
    "        label_samples.append(label)\n",
    "\n",
    "    sequence_lengths = [len(tok) for tok in token_samples]\n",
    "    max_seq_len = max(sequence_lengths)\n",
    "\n",
    "    padding_masks = []\n",
    "    for idx in range(len(token_samples)):\n",
    "        sample = token_samples[idx]\n",
    "        seq_len = len(sample)\n",
    "        diff = max_seq_len - seq_len\n",
    "\n",
    "        if diff > 0:\n",
    "\n",
    "            padding = torch.tensor([special_token_idx[\"<pad>\"] for _ in range(diff)])\n",
    "            sample = torch.concatenate((sample, padding))\n",
    "            token_samples[idx] = sample\n",
    "            \n",
    "            label_padding = torch.tensor([-100 for _ in range(diff)])\n",
    "            label_samples[idx] = torch.concatenate((label_samples[idx], label_padding))\n",
    "\n",
    "            padding_mask = (sample==special_token_idx[\"<pad>\"])\n",
    "            padding_masks.append(padding_mask)\n",
    "\n",
    "        else:\n",
    "            padding_masks.append(torch.zeros(max_seq_len))\n",
    "\n",
    "    token_samples = torch.stack(token_samples)\n",
    "    label_samples = torch.stack(label_samples)\n",
    "    padding_masks = torch.stack(padding_masks)\n",
    "\n",
    "    assert token_samples.shape == label_samples.shape == padding_masks.shape\n",
    "    \n",
    "    batch = {\"input_ids\": token_samples, \n",
    "             \"labels\": label_samples, \n",
    "             \"attention_mask\": padding_masks.bool()}\n",
    "\n",
    "    return batch\n",
    "    \n",
    "dataloader = DataLoader(mlm, batch_size=16, collate_fn=collate_fn)\n",
    "\n",
    "for batch in dataloader:\n",
    "    print(batch[\"input_ids\"])\n",
    "    print(batch[\"labels\"])\n",
    "    print(batch[\"attention_mask\"])\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
