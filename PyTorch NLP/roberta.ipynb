{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c53a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaTokenizer, get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ea108b",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e732c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "\n",
    "print(\"Special Tokens in Roberta Tokenizer\")\n",
    "special_tokens = tokenizer.special_tokens_map\n",
    "print(special_tokens)\n",
    "\n",
    "\n",
    "special_token_idx = {}\n",
    "for token in special_tokens.values():\n",
    "    special_token_idx[token] = tokenizer.encode(token,add_special_tokens = False)[0]\n",
    "\n",
    "print(f\"Special Token Index: {special_token_idx}\")\n",
    "\n",
    "all_tokens_idx = list(range(tokenizer.vocab_size))\n",
    "all_special_tokens_idx = sorted(list(special_token_idx.values()))\n",
    "all_non_special_tokens_idx = [token for token in all_tokens_idx if token not in all_special_tokens_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183de5a2",
   "metadata": {},
   "source": [
    "### Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec520983",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = \"./data/harry_potter_txt/\"\n",
    "\n",
    "text_files = os.listdit(path_to_data)\n",
    "\n",
    "all_text = \"\"\n",
    "for book in text_files:\n",
    "    with open(os.path.join(path_to_data,book),\"r\") as f:\n",
    "        text = f.readlines()\n",
    "        text = [line for line in text if \"Page\" not in line]\n",
    "        text = \" \".join(text).replace(\"\\n\", \"\")\n",
    "        text = [word for word in text.split(\" \") if len(word)>0]\n",
    "        text = \" \".join(text)\n",
    "        all_text += text\n",
    "all_text = all_text.split(\".\") \n",
    "\n",
    "all_text_chunked = [\".\".join(all_text[i:i+5]) for i in range(0,len(all_text),5)]\n",
    "\n",
    "tokenized_text = [tokenizer.encode(text) for text in all_text_chunked]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06984a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLMLoader(Dataset):\n",
    "    def __init__(self, tokenized_data, max_seq_len=100, masking_ratio=0.15):\n",
    "        self.data = tokenized_data\n",
    "        self.mask_ratio = masking_ratio\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _random_mask_text(self, tokens):\n",
    "        random_masking = torch.rand(*tokens.shape)\n",
    "\n",
    "        special_tokens = torch.tensor(tokenizer.get_special_tokens_mask(tokens, already_has_special_tokens=True))\n",
    "        random_masking[special_tokens==1] = 1\n",
    "\n",
    "        random_masking = (random_masking < self.mask_ratio)\n",
    "\n",
    "        labels = torch.full((tokens.shape), -100)\n",
    "        labels[random_masking] = tokens[random_masking]\n",
    "\n",
    "        random_selected_idx = random_masking.nonzero()\n",
    "\n",
    "        masking_flag = torch.rand(*random_selected_idx.shape)\n",
    "        masking_flag = (masking_flag<0.8)\n",
    "        selected_idx_for_masking = random_selected_idx[masking_flag]\n",
    "\n",
    "        unselected_idx_for_masking = random_selected_idx[~masking_flag]\n",
    "\n",
    "        masking_flag = torch.rand(*unselected_idx_for_masking.shape)\n",
    "        masking_flag = (masking_flag<0.5)\n",
    "        selected_idx_for_random_filling = unselected_idx_for_masking[masking_flag]\n",
    "        selected_idx_to_be_left_alone = unselected_idx_for_masking[~masking_flag]\n",
    "        \n",
    "        if len(selected_idx_for_masking) > 0:\n",
    "            tokens[selected_idx_for_masking] = special_token_idx[\"<mask>\"]\n",
    "        \n",
    "        if len(selected_idx_for_random_filling) > 0:\n",
    "            randomly_selected_tokens = torch.tensor(random.sample(all_non_special_tokens_idx, len(selected_idx_for_random_filling)))\n",
    "            tokens[selected_idx_for_random_filling] = randomly_selected_tokens\n",
    "        \n",
    "        \n",
    "        return tokens, labels\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        data = torch.tensor(self.data[idx])\n",
    "\n",
    "        if len(data) > self.max_seq_len:\n",
    "            rand_start_idx = random.choice(list(range(len(data) - self.max_seq_len)))\n",
    "            end_idx = rand_start_idx + self.max_seq_len\n",
    "            data = data[rand_start_idx:end_idx]\n",
    "  \n",
    "        masked_tokens, label = self._random_mask_text(data)\n",
    "\n",
    "        return masked_tokens, label\n",
    "\n",
    "mlm = MaskedLMLoader(tokenized_text)\n",
    "\n",
    "for masked_tokens, labels in mlm:\n",
    "    print(masked_tokens)\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831250e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    token_samples = []\n",
    "    label_samples =[]\n",
    "\n",
    "    for token, label in batch:\n",
    "        token_samples.append(token)\n",
    "        label_samples.append(label)\n",
    "\n",
    "    sequence_lengths = [len(tok) for tok in token_samples]\n",
    "    max_seq_len = max(sequence_lengths)\n",
    "\n",
    "    padding_masks = []\n",
    "    for idx in range(len(token_samples)):\n",
    "        sample = token_samples[idx]\n",
    "        seq_len = len(sample)\n",
    "        diff = max_seq_len - seq_len\n",
    "\n",
    "        if diff > 0:\n",
    "\n",
    "            padding = torch.tensor([special_token_idx[\"<pad>\"] for _ in range(diff)])\n",
    "            sample = torch.concatenate((sample, padding))\n",
    "            token_samples[idx] = sample\n",
    "            \n",
    "            label_padding = torch.tensor([-100 for _ in range(diff)])\n",
    "            label_samples[idx] = torch.concatenate((label_samples[idx], label_padding))\n",
    "\n",
    "            padding_mask = (sample==special_token_idx[\"<pad>\"])\n",
    "            padding_masks.append(padding_mask)\n",
    "\n",
    "        else:\n",
    "            padding_masks.append(torch.zeros(max_seq_len))\n",
    "\n",
    "    token_samples = torch.stack(token_samples)\n",
    "    label_samples = torch.stack(label_samples)\n",
    "    padding_masks = torch.stack(padding_masks)\n",
    "\n",
    "    assert token_samples.shape == label_samples.shape == padding_masks.shape\n",
    "    \n",
    "    batch = {\"input_ids\": token_samples, \n",
    "             \"labels\": label_samples, \n",
    "             \"attention_mask\": padding_masks.bool()}\n",
    "\n",
    "    return batch\n",
    "    \n",
    "dataloader = DataLoader(mlm, batch_size=16, collate_fn=collate_fn)\n",
    "\n",
    "for batch in dataloader:\n",
    "    print(batch[\"input_ids\"])\n",
    "    print(batch[\"labels\"])\n",
    "    print(batch[\"attention_mask\"])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee9d108",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionEncoder(nn.Module):\n",
    "  def __init__(self,\n",
    "               embed_dim=768,\n",
    "               num_heads=12, \n",
    "               attn_p=0,\n",
    "               proj_p=0):\n",
    "\n",
    "    super(SelfAttentionEncoder, self).__init__()\n",
    "    assert embed_dim % num_heads == 0\n",
    "    self.num_heads = num_heads\n",
    "    self.head_dim = int(embed_dim / num_heads)\n",
    "    self.scale = self.head_dim ** -0.5\n",
    "\n",
    "    self.qkv = nn.Linear(embed_dim, embed_dim*3)\n",
    "    self.attn_p = attn_p\n",
    "    self.attn_drop = nn.Dropout(attn_p)\n",
    "    self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "    self.proj_drop = nn.Dropout(proj_p)\n",
    "\n",
    "  def forward(self, x, attention_mask=None):\n",
    "    batch_size, seq_len, embed_dim = x.shape\n",
    "    qkv = self.qkv(x).reshape(batch_size, seq_len, 3, self.num_heads, self.head_dim)\n",
    "    qkv = qkv.permute(2,0,3,1,4)\n",
    "    q,k,v = qkv.unbind(0)\n",
    "\n",
    "    attn = (q @ k.transpose(-2,-1)) * self.scale\n",
    "\n",
    "    if attention_mask is not None:\n",
    "      \n",
    "        attention_mask = attention_mask.unsqueeze(1).unsqueeze(1)\n",
    "        attn = attn.masked_fill(attention_mask, float('-inf'))\n",
    "\n",
    "\n",
    "    attn = attn.softmax(dim=-1)\n",
    "    attn = self.attn_drop(attn)\n",
    "    x = attn @ v\n",
    "\n",
    "    x = x.transpose(1,2).reshape(batch_size, seq_len, embed_dim)\n",
    "    x = self.proj(x)\n",
    "    x = self.proj_drop(x)\n",
    "      \n",
    "    return x\n",
    "\n",
    "rand_x = torch.randn(2,5,16)\n",
    "padding = torch.tensor([[False, False, False, True, True], \n",
    "                        [False, False, False, False, False]])\n",
    "\n",
    "a = SelfAttentionEncoder(embed_dim=16, num_heads=4)\n",
    "out = a(rand_x, padding)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1ce8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_features,\n",
    "                 hidden_features,\n",
    "                 out_features,\n",
    "                 act_layer=nn.GELU,\n",
    "                 mlp_p=0):\n",
    "\n",
    "\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.drop1 = nn.Dropout(mlp_p)\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop2 = nn.Dropout(mlp_p)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, \n",
    "                 embed_dim=768, \n",
    "                 num_heads=12, \n",
    "                 mlp_ratio=4, \n",
    "                 proj_p=0., \n",
    "                 attn_p=0., \n",
    "                 mlp_p=0., \n",
    "                 act_layer=nn.GELU, \n",
    "                 norm_layer=nn.LayerNorm):\n",
    "\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(embed_dim, eps=1e-6)\n",
    "        self.attn = SelfAttentionEncoder(embed_dim=embed_dim,\n",
    "                                         num_heads=num_heads, \n",
    "                                         attn_p=attn_p,\n",
    "                                         proj_p=proj_p)\n",
    "\n",
    "\n",
    "        self.norm2 = norm_layer(embed_dim, eps=1e-6)\n",
    "        self.mlp = MLP(in_features=embed_dim,\n",
    "                       hidden_features=int(embed_dim*mlp_ratio),\n",
    "                       out_features=embed_dim,\n",
    "                       act_layer=act_layer,\n",
    "                       mlp_p=mlp_p)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        x = x + self.attn(self.norm1(x), attention_mask)\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcb3440",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Roberta(nn.Module):\n",
    "    def __init__(self,\n",
    "                 max_seq_len = 512,\n",
    "                 vocab_size = tokenizer.vocab_size,\n",
    "                 embed_dim = 768,\n",
    "                 depth = 12,\n",
    "                 num_heads = 12,\n",
    "                 mlp_ratio = 4,\n",
    "                 attn_p = 0.,\n",
    "                 mlp_p = 0.,\n",
    "                 proj_p = 0.,\n",
    "                 pos_p = 0.,\n",
    "                 act_layer = nn.GELU,\n",
    "                 norm_layer = nn.LayerNorm):\n",
    "        super(Roberta,self).__init__()\n",
    "\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.embeddings = nn.Embedding(vocab_size,embed_dim)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1,1,embed_dim))\n",
    "        self.pos_embed = nn.Embedding(max_seq_len+1,embed_dim)\n",
    "        self.pos_drop = nn.Dropout(pos_p)\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                Block(\n",
    "                    embed_dim=embed_dim,\n",
    "                    num_heads=num_heads,\n",
    "                    mlp_ratio=mlp_ratio,\n",
    "                    proj_p=proj_p,\n",
    "                    attn_p=attn_p,\n",
    "                    mlp_p=mlp_p,\n",
    "                    act_layer=act_layer,\n",
    "                    norm_layer=norm_layer\n",
    "                )\n",
    "                for _ in range(depth)\n",
    "            ]\n",
    "        )\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim,vocab_size)\n",
    "\n",
    "    def forward(self,x,attention_mask):\n",
    "        device = x.device\n",
    "\n",
    "        batch_size , seq_len = x.shape\n",
    "\n",
    "        if seq_len> self.max_seq_len:\n",
    "            x = x[:,-self.max_seq_len:]\n",
    "        \n",
    "        avail_idx = torch.range(0,seq_len+1,dtype = torch.long,device = device)\n",
    "\n",
    "        tok_emb = self.embeddings(x)\n",
    "\n",
    "        cls_token = self.cls_token.expand(batch_size,-1,-1)\n",
    "        tok_emb = torch.cat((cls_token,tok_emb),dim = 1)\n",
    "\n",
    "        pos_emb = self.pos_embed(avail_idx)\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        cls_token_final = x[:,0]\n",
    "        x = x[:,1:]\n",
    "        x = self.head(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    rand_x = torch.randint(0,10,(2,5))\n",
    "    padding = torch.tensor([[False,False,False,True,True],\n",
    "                            [False,False,False,False,False]])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbce5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Roberta()\n",
    "out = model(rand_x,padding)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c426535",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 15000\n",
    "max_len = 100\n",
    "evaluate_interval = 100\n",
    "embedding_dim = 384\n",
    "depth = 4\n",
    "num_heads = 4\n",
    "lr = 0.0005\n",
    "batch_size = 64\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = Roberta(max_seq_len=max_len, \n",
    "                embed_dim=embedding_dim, \n",
    "                depth=depth, \n",
    "                num_heads=num_heads, \n",
    "                attn_p=0.1, \n",
    "                mlp_p=0.1, \n",
    "                proj_p=0.1, \n",
    "                pos_p=0.1)\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "dataset = MaskedLMLoader(tokenized_text, max_seq_len=max_len)\n",
    "trainset, testset = torch.utils.data.random_split(dataset, [int(0.95*len(dataset)),int(len(dataset) - int(0.95*len(dataset)))])\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer=optimizer, \n",
    "                                            num_warmup_steps=1500, \n",
    "                                            num_training_steps=iterations)\n",
    "\n",
    "\n",
    "train = True\n",
    "completed_steps = 0\n",
    "all_training_losses, all_validation_losses = [], []\n",
    "training_losses, validation_losses = [], []\n",
    "progress_bar = tqdm(range(iterations))\n",
    "\n",
    "while train:\n",
    "\n",
    "    for batch in trainloader:\n",
    "\n",
    "        inputs, labels, mask = batch[\"input_ids\"], batch[\"labels\"], batch[\"attention_mask\"]\n",
    "        inputs, labels, mask = inputs.to(DEVICE), labels.to(DEVICE), mask.to(DEVICE)\n",
    "\n",
    "        prediction = model(inputs, mask)\n",
    "\n",
    "        prediction = prediction.reshape(-1, prediction.shape[-1]) \n",
    "        labels = labels.reshape(-1) \n",
    "        loss = loss_fn(prediction, labels)\n",
    "        training_losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        completed_steps += 1\n",
    "        progress_bar.update(1)\n",
    "            \n",
    "        if completed_steps % evaluate_interval == 0:\n",
    "            with torch.no_grad():\n",
    "                for batch in testloader:\n",
    "                    inputs, labels, mask = batch[\"input_ids\"], batch[\"labels\"], batch[\"attention_mask\"]\n",
    "                    inputs, labels, mask = inputs.to(DEVICE), labels.to(DEVICE), mask.to(DEVICE)\n",
    "                    prediction = model(inputs, mask)\n",
    "                    prediction = prediction.reshape(-1, prediction.shape[-1]) \n",
    "                    labels = labels.reshape(-1) \n",
    "                    loss = loss_fn(prediction, labels)\n",
    "                    validation_losses.append(loss.item())\n",
    "\n",
    "            avg_training_loss = np.mean(training_losses)\n",
    "            avg_eval_loss = np.mean(validation_losses)\n",
    "            \n",
    "            print(f\"Iteration {completed_steps} Training Loss:\", avg_training_loss, \"Validation Loss:\", avg_eval_loss)\n",
    "\n",
    "            all_training_losses.append(avg_training_loss)\n",
    "            all_validation_losses.append(avg_eval_loss)\n",
    "\n",
    "            training_losses = []\n",
    "            validation_losses = []\n",
    "\n",
    "        if completed_steps >= iterations:\n",
    "            train = False\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e456324",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(all_training_losses)\n",
    "plt.plot(all_validation_losses)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4bffbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_in_the_gap(masked_sentence, k=3):\n",
    "    model.eval()\n",
    "    encoded_text = tokenizer.encode(masked_sentence)\n",
    "    input_tokens = torch.tensor(encoded_text).to(DEVICE)\n",
    "\n",
    "    masked_token_index = (input_tokens == special_token_idx[\"<mask>\"]).nonzero().item()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tokens.unsqueeze(0), attention_mask=None)\n",
    "\n",
    "    predicted_output = output[0, masked_token_index]\n",
    "\n",
    "    top_k_predicted = torch.topk(predicted_output, k=k).indices\n",
    "\n",
    "    predicted_words = tokenizer.decode(top_k_predicted.tolist()).strip().split(\" \")\n",
    "\n",
    "    return predicted_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe6b032",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The Wizarding <mask> is a wonderful place to visit\"\n",
    "fill_in_the_gap(masked_sentence=sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5146556f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Lord Voldemort is a <mask> wizard\"\n",
    "fill_in_the_gap(masked_sentence=sentence)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
