{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c53a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaTokenizer, get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ea108b",
   "metadata": {},
   "source": [
    "### TOkenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e732c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "\n",
    "print(\"Special Tokens in Roberta Tokenizer\")\n",
    "special_tokens = tokenizer.special_tokens_map\n",
    "print(special_tokens)\n",
    "\n",
    "\n",
    "special_tokens_idx = {}\n",
    "for token in special_tokens.values():\n",
    "    special_tokens_idx[token] = tokenizer.encode(token,add_special_tokens = False)[0]\n",
    "\n",
    "print(f\"Special Token Index: {special_tokens_idx}\")\n",
    "\n",
    "all_tokens_idx = list(range(tokenizer.vocab_size))\n",
    "all_special_tokens_idx = sorted(list(special_tokens_idx.values()))\n",
    "all_non_special_tokens_idx = [token for token in all_tokens_idx if token not in all_special_tokens_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183de5a2",
   "metadata": {},
   "source": [
    "### Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec520983",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = \"./data/harry_potter_txt/\"\n",
    "\n",
    "text_files = os.listdit(path_to_data)\n",
    "\n",
    "all_text = \"\"\n",
    "for book in text_files:\n",
    "    with open(os.path.join(path_to_data,book),\"r\") as f:\n",
    "        text = f.readlines()\n",
    "        text = [line for line in text if \"Page\" not in line]\n",
    "        text = \" \".join(text).replace(\"\\n\", \"\")\n",
    "        text = [word for word in text.split(\" \") if len(word)>0]\n",
    "        text = \" \".join(text)\n",
    "        all_text += text\n",
    "all_text = all_text.split(\".\") \n",
    "\n",
    "all_text_chunked = [\".\".join(all_text[i:i+5]) for i in range(0,len(all_text),5)]\n",
    "\n",
    "tokenized_text = [tokenizer.encode(text) for text in all_text_chunked]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
