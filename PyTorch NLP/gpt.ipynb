{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdc7410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import GPT2TokenizerFast\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310ee985",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = \"./data/harry_potter.txt/\"\n",
    "\n",
    "text_files = os.listdir(path_to_data)\n",
    "\n",
    "all_text = \"\"\n",
    "for book in text_files:\n",
    "    with open(os.path.join(path_to_data,book),\"r\") as f:\n",
    "        text = f.readlines()\n",
    "        text = [line for line in text if \"page\" not in line]\n",
    "        text = \" \".join(text).replace(\"\\n\",\"\")\n",
    "        text = [word for word in text.split(\" \") if len(word)>0]\n",
    "        text = \" \".join(text)\n",
    "        all_text += text\n",
    "    \n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "print(f\"Tokenizer vocab size: {tokenizer.vocab_size}\")\n",
    "\n",
    "tokenized_data = tokenizer(all_text)[\"input_ids\"]\n",
    "print(f\"Number of Tokens: {len(tokenized_data)}\")\n",
    "\n",
    "print(f\"Example Token: {tokenized_data[:30]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dc69e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataBuilder:\n",
    "    def __init__(self,seq_len = 300 , tokenized_text = tokenized_data):\n",
    "        self.seq_len = seq_len +1\n",
    "        self.tokenized_text = tokenized_text\n",
    "        self.file_length = len(tokenized_text)\n",
    "\n",
    "    def grab_random_sample(self):\n",
    "        start = np.random.randin(0,len(self.tokenized_text)- self.seq_len)\n",
    "        end = start + self.seq_len\n",
    "        text_slice = self.tokenized_text[start:end]\n",
    "\n",
    "        input_text = torch.tensor(text_slice[:-1])\n",
    "        label = torch.tensor(text_slice[1:])\n",
    "\n",
    "        return input_text,label\n",
    "    \n",
    "    def grab_random_batch(self,batch_size):\n",
    "        input_texts , labels = [], []\n",
    "        for _ in range(batch_size):\n",
    "            input_text , label = self.grab_random_sample()\n",
    "            input_texts.append(input_text)\n",
    "            labels.append(label)\n",
    "        \n",
    "        input_texts = torch.stack(input_texts)\n",
    "        labels = torch.stack(labels)\n",
    "\n",
    "        return input_texts , labels\n",
    "    \n",
    "dataset = DataBuilder(tokenized_text=tokenized_data, seq_len= 5)\n",
    "input_texts , labels = dataset.grab_random_batch(batch_size=2)\n",
    "\n",
    "print(f\"Input Text: {input_texts.shape}\")\n",
    "print(input_texts)\n",
    "\n",
    "print(f\"Label Text: {labels.shape}\")\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10c47e6",
   "metadata": {},
   "source": [
    "### Casual masking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814def40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CasualMasking(seq_len):\n",
    "    ones = torch.ones((seq_len,seq_len))\n",
    "    casual_mask = torch.tril(ones)\n",
    "    casual_mask = casual_mask.reshape(1,1,seq_len,seq_len).bool()\n",
    "    return casual_mask\n",
    "\n",
    "casual_mask = CasualMasking(5)\n",
    "\n",
    "print(f\"Casual Mask: {casual_mask.shape}\")\n",
    "print(casual_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cffe18",
   "metadata": {},
   "source": [
    "### Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a85a43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionDecoder(nn.Module):\n",
    "    def __init__(self,seq_len = 300, embed_dim = 768, num_heads = 12, attn_p = 0 ,proj_p = 0):\n",
    "        super(SelfAttentionDecoder,self).__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = int(embed_dim/num_heads)\n",
    "        self.scale = self.head_dim ** 0.5\n",
    "\n",
    "        self.qkv = nn.Linear(embed_dim,embed_dim*3)\n",
    "        self.attn_p = attn_p\n",
    "        self.attn_drop = nn.Dropout(attn_p)\n",
    "        self.proj = nn.Linear(embed_dim,embed_dim)\n",
    "        self.proj_drop = nn.Dropout(proj_p)\n",
    "\n",
    "        self.register_buffer(\"casual_mask\",CasualMasking(seq_len=seq_len).to(torch.bool))\n",
    "\n",
    "    def forward(self,x):\n",
    "        batch_size , seq_len , embed_dim = x.shape\n",
    "        qkv = self.qkv(x).reshape(batch_size,seq_len,3,self.num_heads,self.head_dim)\n",
    "        qkv = qkv.permute(1,0,3,1,4)\n",
    "        q,k,v = qkv.unbind(0)\n",
    "\n",
    "        attn = ( q @ k.transpose(-2,-1)) * self.scale\n",
    "\n",
    "        attn = attn.masked_fill(self.casual_mask[:,:,:seq_len,:seq_len]==0,float('inf'))\n",
    "        attn = attn.softmax(dim = 1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        x = attn @ v\n",
    "\n",
    "        x = x.transpose(1,2).reshape(batch_size,seq_len,embed_dim)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "    \n",
    "x = torch.randn(2,300,768)\n",
    "a = SelfAttentionDecoder()\n",
    "out = a(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643408c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features,hidden_features,out_features,act_layer=nn.GELU,mlp_p=0):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer() \n",
    "        self.drop1 = nn.Dropout(mlp_p)\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop2 = nn.Dropout(mlp_p)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop2(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, seq_len=300, embed_dim=768, num_heads=12, mlp_ratio=4, proj_p=0., attn_p=0., mlp_p=0.,act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(embed_dim, eps=1e-6)\n",
    "        self.attn = SelfAttentionDecoder(seq_len=seq_len,\n",
    "                                         embed_dim=embed_dim,\n",
    "                                         num_heads=num_heads, \n",
    "                                         attn_p=attn_p,\n",
    "                                         proj_p=proj_p)\n",
    "\n",
    "\n",
    "        self.norm2 = norm_layer(embed_dim, eps=1e-6)\n",
    "        self.mlp = MLP(in_features=embed_dim,\n",
    "                       hidden_features=int(embed_dim*mlp_ratio),\n",
    "                       out_features=embed_dim,\n",
    "                       act_layer=act_layer,\n",
    "                       mlp_p=mlp_p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd06f4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, max_seq_len=512, vocab_size=tokenizer.vocab_size,embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, attn_p=0., mlp_p=0., proj_p=0., pos_p=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_embed = nn.Embedding(max_seq_len, embed_dim)\n",
    "        self.pos_drop = nn.Dropout(pos_p)\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                Block(seq_len=max_seq_len, \n",
    "                      embed_dim=embed_dim, \n",
    "                      num_heads=num_heads, \n",
    "                      mlp_ratio=mlp_ratio, \n",
    "                      proj_p=proj_p, \n",
    "                      attn_p=attn_p, \n",
    "                      mlp_p=mlp_p, \n",
    "                      act_layer=act_layer, \n",
    "                      norm_layer=norm_layer)\n",
    "\n",
    "                for _ in range(depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, vocab_size)\n",
    "        self.embeddings.weight = self.head.weight\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.trunc_normal_(module.weight, std=0.02, a=-2, b=2)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.trunc_normal_(module.weight, std=0.02, a=-2, b=2)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "\n",
    "        batch_size, seq_len = x.shape\n",
    "\n",
    "        avail_idx = torch.arange(0, seq_len, dtype=torch.long, device=device)\n",
    "\n",
    "        tok_emb = self.embeddings(x)\n",
    "        pos_emb = self.pos_embed(avail_idx)\n",
    "\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def write(self, input_tokens, max_new_tokens, temperature=1.0, sample=True):\n",
    "        for i in range(max_new_tokens):\n",
    "            idx_cond = input_tokens if input_tokens.shape[1] < self.max_seq_len else input_tokens[:, -self.max_seq_len:]\n",
    "            logits = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            if sample:\n",
    "                idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                idx_next = torch.argmax(probs, dim=-1).unsqueeze(0)\n",
    "            input_tokens = torch.cat([input_tokens, idx_next], dim=-1)\n",
    "        return input_tokens.detach().cpu().numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
