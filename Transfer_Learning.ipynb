{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20db1315",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models import AlexNet , AlexNet_Weights\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7716b041",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = \"/dogsvscats/\"\n",
    "\n",
    "normalizer = transforms.Normalize(\n",
    "    mean = [0.485, 0.456, 0.406],\n",
    "    std = [0.229,0.224,0.225]\n",
    ")\n",
    "\n",
    "train_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.RandomForizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalizer\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset = ImageFolder(path_to_data,transform= train_transforms)\n",
    "\n",
    "train_samples, test_samples = int(0.9*len(dataset)) , len(dataset)- int(0.9*len(dataset))\n",
    "train_dataset , val_dataset = torch.utils.data.random_split(dataset, lengths = [train_samples,test_samples])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c476df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AlexNet()\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabde3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b70030",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AlexNet()\n",
    "model.classifier[6] = nn.Linear(4096,2)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0a06be",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_data = torch.rand(16,3,224,224)\n",
    "model_output = model(rand_data)\n",
    "model_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af14f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Parameters\n",
    "total_parameters = 0\n",
    "for name , params in model.named_parameters():\n",
    "    num_params = int(torch.prod(torch.tensor(params.shape)))\n",
    "    print(f\"{name} : {params.shape}, Num Parameters: {num_params}\")\n",
    "    total_parameters += num_params\n",
    "print(\"----------------------\")\n",
    "print(f\"Total Parameters in AlexNet Model: {total_parameters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a31d235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training from Scratch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = AlexNet()\n",
    "model.classifier[6] = nn.Linear(4096,2)\n",
    "model = model.to(device)\n",
    "\n",
    "num_epochs = 5\n",
    "optimizer = optim.Adam(params = model.parameters(), lr = 0.00001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "batch_size = 128\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "val_loader = DataLoader(val_dataset,batch_size = batch_size,shuffle = False)\n",
    "\n",
    "def train(model, device, num_epochs, optimizer, loss_fn , batch_size, train_loader, val_loader):\n",
    "    log_training = {\n",
    "        \"epochs\": [],\n",
    "        \"training_loss\": [],\n",
    "        \"training_accuracy\": [],\n",
    "        \"validation_loss\": [],\n",
    "        \"validation_accuracy\": []\n",
    "    }\n",
    "\n",
    "    for epoch in range(1,num_epochs+1):\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        training_losses , training_accuracies = [], []\n",
    "        validation_losses, validation_accuracies = [], []\n",
    "\n",
    "        for image,label in tqdm(train_loader):\n",
    "            image, label = image.to(device), label.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model.forward(image)\n",
    "\n",
    "            loss = loss_fn(label,output)\n",
    "            training_losses.append(loss.item())\n",
    "\n",
    "            predictions = torch.argmax(output,axis = 1)\n",
    "            accuracy = (predictions==label).sum() / len(predictions)\n",
    "            training_accuracies.append(accuracy.item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        for image,label in tqdm(val_loader):\n",
    "            image, label = image.to(device) , label.to(device)\n",
    "            with torch.no_grad():\n",
    "                output = model.forward(image)\n",
    "\n",
    "                loss = loss_fn(output,label)\n",
    "                validation_losses.append(loss.item())\n",
    "\n",
    "                predictions = torch.argmax(output,axis = 1)\n",
    "                accuracy = (predictions==label).sum() / len(predictions)\n",
    "                validation_accuracies.append(accuracy.item())\n",
    "            \n",
    "        training_loss_mean , training_accuracy_mean = np.mean(training_losses), np.mean(training_accuracies)\n",
    "        validation_loss_mean , validation_accuracy_mean = np.mean(validation_losses) , np.mean(validation_accuracies)\n",
    "\n",
    "        log_training[\"epochs\"].append(epoch)\n",
    "        log_training[\"training_loss\"].append(training_loss_mean)\n",
    "        log_training[\"training_accuracy\"].append(training_accuracy_mean)\n",
    "        log_training[\"validation_loss\"].append(validation_loss_mean)\n",
    "        log_training[\"validation_accuracy\"].append(validation_accuracy_mean)\n",
    "\n",
    "        print(\"Training Loss: \", training_loss_mean)\n",
    "        print(\"Training Accuracy: \", training_accuracy_mean)\n",
    "        print(\"Validation Loss: \", validation_loss_mean)\n",
    "        print(\"Validation Accuracy: \", validation_accuracy_mean)\n",
    "    \n",
    "    return log_training , model\n",
    "\n",
    "random_init_logs, model = train(\n",
    "    model = model,\n",
    "    device= device,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn= loss_fn,\n",
    "    batch_size=batch_size,\n",
    "    train_loader= train_loader,\n",
    "    val_loader=val_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e6ab55",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.10.0','alexnet',pretrained = True)\n",
    "model.classifier[6] = nn.Linear(4096,2)\n",
    "model = model.to(device)\n",
    "\n",
    "num_epochs = 2\n",
    "optimizer = optim.Adam(params = model.parameters(),lr=0.0001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "batch_size = 128\n",
    "\n",
    "train_loader = DataLoader(train_dataset,batch_size = batch_size,shuffle = True)\n",
    "val_loader = DataLoader(val_dataset,batch_size = batch_size,shuffle = False)\n",
    "\n",
    "random_init_logs , model = train(\n",
    "    model = model,\n",
    "    device = device,\n",
    "    epochs = num_epochs,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=loss_fn,\n",
    "    batch_size=batch_size,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6a4179",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name , param in model.named_parameters():\n",
    "    if \"bias\" in name:\n",
    "        print(name)\n",
    "        print(param)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c28c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name , param in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30181dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=True)\n",
    "model.classifier[6] = nn.Linear(4096,2)\n",
    "\n",
    "for name , param in model.named_parameters():\n",
    "    if \"classifier.6\" not in name:\n",
    "        params.requires_grad_(False)\n",
    "    \n",
    "for name , param in model.named_parameters():\n",
    "    if 'bias' in name:\n",
    "        print(name)\n",
    "        print(param)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c664fa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=True)\n",
    "model.classifier[6] = nn.Linear(4096, 2)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if \"classifier.6\" not in name:\n",
    "        param.requires_grad_(False) \n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "epochs = 2\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=0.0001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "batch_size = 128\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "random_init_logs, model = train(model=model,\n",
    "                                device=device,\n",
    "                                epochs=epochs,\n",
    "                                optimizer=optimizer,\n",
    "                                loss_fn=loss_fn,\n",
    "                                batch_size=batch_size,\n",
    "                                trainloader=train_loader,\n",
    "                                valloader=val_loader)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
